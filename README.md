The notebook Starter_Code.ipynb contains the project code. Trained_model.h5 is the file with the model. DeepLearningQAandSummary.pdf is the document with the answers to questions formulated during the project and a summary at the end. 

The Alphabet Soup Deep Learning Challenge is a machine learning project aimed at helping a nonprofit organization, Alphabet Soup, decide which funding applicants are most likely to succeed. Using historical data on more than 34,000 funded organizations, we build a machine learning model to predict whether future applicants will make good use of their funding. The dataset includes various details about each applicant, such as their organization type, funding amount requested, and special considerations. By analyzing this data, we can create a model that helps Alphabet Soup allocate its resources more effectively.

To achieve this, we use deep learning techniques with TensorFlow and Keras to build a neural network classifier. First, we preprocess the data by cleaning it, encoding categorical variables, and scaling numerical values. Then, we design and train a neural network to classify applicants as either successful or unsuccessful. We experiment with different numbers of layers, neurons, and activation functions to improve model accuracy. If the initial model does not perform well enough, we optimize it by adjusting its architecture and training settings to increase its predictive power.

Finally, we evaluate the model’s performance and save the trained model for future use. We also write a report summarizing our findings, explaining the data preprocessing steps, the neural network design, and ways to improve the model. This project demonstrates how artificial intelligence can be applied to real-world decision-making, helping organizations like Alphabet Soup make better funding choices based on data-driven insights.

Some of the code used during this challenge came from templates from past assignments done during class. Some other parts were done with the help of AI tools like ChatGPT, specifically, the sections where rare categories in some columns needed to be merged into “Other” to reduce dimensionality and improve model performance. AI models were also used for debugging.

